{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#from autograd import numpy as np\n",
    "import bayesian_pdes as bpdes\n",
    "from scipy import stats, linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "import os\n",
    "import time\n",
    "import eit\n",
    "import uuid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic, needs_local_scope\n",
    "@needs_local_scope\n",
    "@register_cell_magic\n",
    "def capture_cell(line, cell):\n",
    "    globals()[line.strip()] = cell\n",
    "    exec(cell, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "This notebook runs the temporal recovery. Parameters of the notebook are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture_cell params\n",
    "# Parameters of the data\n",
    "scale = 1000 # Scaling multiplier applied to the observed data.\n",
    "stim_current = 1.0 # The stimulating current that is applied.\n",
    "likelihood_variance = 1.0 # The observation noise variance.\n",
    "\n",
    "\n",
    "# Parameters controlling the prior\n",
    "kernel_variance_mult = 1000.0 # The amplitude parameter on the prior covariance over the PDE solution.\n",
    "kernel_ls = 0.21109375 # The length-scale parameter on the prior covariance over the PDE solution.\n",
    "conductivity_variance_mult = 1.0 # The amplitude parameter on the prior covariance for the conductivity field.\n",
    "conductivity_ls = 0.3 # The length-scale parameter on the prior covariance for the conductivity field.\n",
    "\n",
    "lamb = 100.0 # The variance parameter on the temporal component of the prior.\n",
    "tau = 0.0 # The shift parameter on the temporal component of the prior.\n",
    "s = 1.0 # The assumed interval at which temporal observations are collected.\n",
    "\n",
    "\n",
    "# Parameters controlling the problem setup\n",
    "n_radii_forward = 7 # The number of radii used in constructing the set of design points for the forward problem.\n",
    "n_radii_inverse = 8 # The number of radii used in constructing the set of design points for the inverse problem.\n",
    "bayesian = True # Whether to use a Bayesian (PN) or non-Bayesian (symmetric collocation) forward solver\n",
    "include_prior_mean = False # Whether to include a prior mean, based on the best constant conductivity field.\n",
    "\n",
    "\n",
    "# Sampling parameters\n",
    "n_particles = 200 # The number of particles in the SMC algorithm.\n",
    "n_iter = 1000 # The number of iterations to perform in each application of the transition kernel for the resampling step.\n",
    "beta = 0.05 # The parameter \\beta in the transition kernel.\n",
    "intermediate_temperatures = 99 # The number of synthetic intermediate tempering distributions\n",
    "n_threads = 8 # Number of threads to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circle(x, y, values, cmap=None):\n",
    "    plt.tricontourf(x.ravel(), y.ravel(), values.ravel(), cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.legend()\n",
    "    plt.axis('scaled')\n",
    "    plt.xlim(-1.1,1.1); plt.ylim(-1.1,1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dest_dir = os.path.join('results', 'temporal_recovery_n={}'.format(n_radii_forward))\n",
    "if not bayesian:\n",
    "    dest_dir += '_collocation'\n",
    "dest_dir += '_' + str(uuid.uuid4())[:8]\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "    os.makedirs(os.path.join(dest_dir, 'detailed_summaries'))\n",
    "print(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dest_dir, 'params.py'), 'w') as f:\n",
    "    f.write(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file format is as follows:\n",
    "* Each row corresponds to a time\n",
    "* The first element of the row gives the time index of the measurement\n",
    "* The remaining columns give the measurements for each time\n",
    "* These are divided into seven blocks.\n",
    "    - Call the block $i$\n",
    "    - Within the block the measurements correspond to electrodes $j=1,\\dots,8$, $j\\neq i$\n",
    "    - Pair $(i,j)$ represents passing a current between electrode $i$ and electrode $j$ and measuring the voltage.\n",
    "    - The voltage is measured between the _reference_ electrode at $1$ and the electrode located at $1+i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for setting up the stim pattern\n",
    "stim_pattern = np.zeros((7,8))\n",
    "for i in range(7):\n",
    "    stim_pattern[i,0] = 1\n",
    "    stim_pattern[i, i+1] = -1\n",
    "stim_pattern = scale*stim_pattern*stim_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_pattern = np.zeros((7,8))\n",
    "meas_pattern[:,0] = 1\n",
    "meas_pattern[:, 1:] = np.diag(-np.ones(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = eit.grids.EITPattern(meas_pattern, stim_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/ST1trial3.DAT'\n",
    "raw_data = np.loadtxt(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(raw_data[:,1:].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(data):\n",
    "    res = {}\n",
    "    for r in data:\n",
    "        time = r[0]\n",
    "        other_data = r[1:]\n",
    "        res[time] = scale*other_data.reshape((7,7))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_to_dict(raw_data)\n",
    "# quick check to make sure the data dict matches expectation\n",
    "assert np.all(data_dict[1][0] == scale*raw_data[0][1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_dict[11]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Forward Solver\n",
    "Here we construct the symbolic representation of the covariance function to pass through to ``bayesian_pdes``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x, s_xbar, s_y, s_ybar = sp.symbols('x,xbar,y,ybar')\n",
    "a, a_x, a_y = sp.symbols('a,a_x,a_y')\n",
    "a_bar, a_x_bar, a_y_bar = sp.symbols('abar,a_xbar,a_ybar')\n",
    "s_length_scale, s_variance = sp.symbols('l,sigma')\n",
    "kernel = s_variance*sp.exp(-((s_x-s_xbar)**2 + (s_y-s_ybar)**2) / (2.*s_length_scale**2))\n",
    "symbols = [[s_x, s_y, a, a_x, a_y], [s_xbar, s_ybar, a_bar, a_x_bar, a_y_bar], [s_length_scale, s_variance]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use:\n",
    "\\begin{align*}\n",
    "    \\nabla \\cdot (\\exp(a)\\nabla u) &= \\nabla \\exp(a) \\cdot \\nabla u + \\exp(a) \\nabla^2 u \\\\\n",
    "    &= \\exp(a) \\left( \\nabla a \\cdot \\nabla u + \\nabla^2 u \\right) \\\\\n",
    "    &= \\exp(a) \\left( a_x u_x + a_y u_y + u_{xx} + u_{yy} \\right)\n",
    "\\end{align*}\n",
    "and:\n",
    "\\begin{align*}\n",
    "    \\exp(a) \\nabla u \\cdot n &= \\exp(a) (x u_x + y u_y)\n",
    "\\end{align*}\n",
    "for the special case of a unit circular domain centered at the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the collocation grid\n",
    "This is the grid used to solve the PDE using symmetric collocation / the PMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_shell = n_radii_forward\n",
    "n_bdy = 32\n",
    "n_sensor = 8\n",
    "grid = eit.grids.construct_circular(n_in_shell, n_bdy, n_sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot design points\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(grid.interior[:,0], grid.interior[:,1], marker='x', color='black', s=10)\n",
    "plt.scatter(grid.boundary[:,0], grid.boundary[:,1], marker='x', color='black', s=10, label='Design point')\n",
    "plt.scatter(grid.sensors[:,0], grid.sensors[:,1], marker='x', color='green', label='Sensor')\n",
    "plt.xlim(1.05,-1.05);plt.ylim(1.05,-1.05)\n",
    "plt.legend(loc=2, bbox_to_anchor=(1.05,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Proposal Matrix\n",
    "First construct the grid on which the posterior will be sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_kernel = kernel\n",
    "def diff_x(k):\n",
    "    return k.diff(s_x)\n",
    "def diff_y(k):\n",
    "    return k.diff(s_y)\n",
    "def diff_x_bar(k):\n",
    "    return k.diff(s_xbar)\n",
    "def diff_y_bar(k):\n",
    "    return k.diff(s_ybar)\n",
    "\n",
    "proposal_op_system = bpdes.operator_compilation.sympy_gram.compile_sympy(\n",
    "    [diff_x, diff_y], \n",
    "    [diff_x_bar, diff_y_bar], \n",
    "    proposal_kernel,\n",
    "    [[s_x, s_y], [s_xbar, s_ybar], [s_length_scale, s_variance]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_design = eit.grids.construct_shell(np.linspace(0,1,n_radii_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(proposal_design[:,0], proposal_design[:,1], marker='x')\n",
    "plt.xlim(1.05,-1.05);plt.ylim(1.05,-1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_design = np.row_stack([grid.interior_plus_boundary, grid.sensors])\n",
    "proposal_fun_args = np.array([conductivity_ls, conductivity_variance_mult])\n",
    "proposal_cov = bpdes.collocation.compute_operator_matrix(\n",
    "    [()],\n",
    "    [()],\n",
    "    [proposal_design],\n",
    "    [proposal_design],\n",
    "    proposal_op_system,\n",
    "    proposal_fun_args\n",
    ")\n",
    "proposal_lhs_mat = bpdes.collocation.compute_operator_matrix(\n",
    "    [(), diff_x, diff_y], \n",
    "    [()],\n",
    "    [full_design, grid.interior, grid.interior],\n",
    "    [proposal_design],\n",
    "    proposal_op_system,\n",
    "    proposal_fun_args\n",
    ")\n",
    "proposal_cov_inv = np.linalg.inv(proposal_cov)\n",
    "np.linalg.cond(proposal_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_dot_mat = np.dot(proposal_lhs_mat, proposal_cov_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly factorise the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = kernel_ls\n",
    "fun_args = np.array([kernel_ls, kernel_variance_mult])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate best constant field, in case a prior mean has been used\n",
    "import scipy.optimize\n",
    "def phi_const(k):\n",
    "    theta = k*np.ones(proposal_cov.shape[0])\n",
    "    return eit.kernels.c_kernel.phi(grid, theta, likelihood_variance, pattern, data_dict[11], fun_args, proposal_dot_mat)\n",
    "    \n",
    "best_const = scipy.optimize.minimize_scalar(phi_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.05\n",
    "if include_prior_mean:\n",
    "    prior_mean = best_const.x*np.ones(proposal_cov.shape[0])\n",
    "else:\n",
    "    prior_mean = np.zeros(proposal_cov.shape[0])\n",
    "sqrt_proposal_cov = np.real_if_close(linalg.sqrtm(proposal_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Run the SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the initial sample from the prior\n",
    "initial_samples = np.random.multivariate_normal(prior_mean, proposal_cov, n_particles)\n",
    "# seed with kappa_0\n",
    "#initial_samples = np.repeat(kappa_0[None, :], n_particles, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc(initial_samples, kernels, resample_threshold, init_beta, debug=False):\n",
    "    # Construct a data frame to store the output\n",
    "    res_df = pd.DataFrame(columns=('Kernel_Name', 'Beta', 'Accept', 'ESS', 'Time'), index=np.arange(len(kernels)))\n",
    "    \n",
    "    # Initial log weights\n",
    "    log_weights = 1 - np.log(initial_samples.shape[0])\n",
    "    weights = 1./initial_samples.shape[0]\n",
    "    ess = initial_samples.shape[0] \n",
    "    cur_sample = initial_samples\n",
    "    \n",
    "    samples = []\n",
    "    beta = init_beta\n",
    "    for i in range(len(kernels)):\n",
    "        # re-sample\n",
    "        if ess < resample_threshold*initial_samples.shape[0]:\n",
    "            if debug: print(\"Resampling\")\n",
    "            resamp_indices = np.random.choice(len(cur_sample), len(cur_sample), p=weights)\n",
    "            cur_sample = np.copy(cur_sample[resamp_indices])\n",
    "            log_weights = np.ones(initial_samples.shape[0]) - np.log(initial_samples.shape[0])\n",
    "        \n",
    "        # evolve\n",
    "        if debug: print(\"Evolving particles\")\n",
    "        kernel = kernels[i]\n",
    "        t = time.time()\n",
    "        new_sample, average_acceptances, log_likelihoods = kernel.apply(cur_sample, n_iter, n_threads=n_threads, beta=beta, bayesian=bayesian)\n",
    "        taken = time.time() - t\n",
    "        log_likelihoods = log_likelihoods.ravel()\n",
    "        average_acceptances = average_acceptances.ravel()\n",
    "        if debug: print(\"Done, it took {:.2f}s\".format(taken))\n",
    "        \n",
    "        # re-weight\n",
    "        last_kernel_log_likes = np.nan*np.empty(new_sample.shape[0])\n",
    "        log_temporal_component = np.empty(new_sample.shape[0])\n",
    "        for j in range(new_sample.shape[0]):\n",
    "            new_samp = new_sample[j]\n",
    "            if i > 0:\n",
    "                last_kernel_log_likes[j] = -kernels[i-1].phi(new_samp, bayesian=bayesian)\n",
    "            old_samp = cur_sample[j]\n",
    "            delta = new_samp - old_samp\n",
    "            log_temporal_component[j] = -1./(2*lamb*(s+tau))*np.dot(delta, np.dot(proposal_cov_inv, delta))\n",
    "\n",
    "        log_spatial_component = log_likelihoods.ravel()\n",
    "        if i > 0:\n",
    "            log_spatial_component = log_spatial_component - last_kernel_log_likes\n",
    "        \n",
    "        log_weights = log_weights + log_spatial_component + log_temporal_component\n",
    "        # log-sum-exp\n",
    "        norm_factor = np.max(log_weights) + np.log(np.sum(np.exp(log_weights - np.max(log_weights))))\n",
    "        log_weights = log_weights - norm_factor\n",
    "        weights = np.exp(log_weights)\n",
    "        \n",
    "        # store output\n",
    "        output = pd.DataFrame({\n",
    "            'K_i accept': average_acceptances,\n",
    "            'K_i log-likelihood': log_likelihoods,\n",
    "            'K_{i-1} log-likelihood': last_kernel_log_likes,\n",
    "            'Spatial update': log_spatial_component,\n",
    "            'Temporal update': log_temporal_component,\n",
    "            'Log-weights': log_weights,\n",
    "            'Weights': weights\n",
    "        })\n",
    "        output.to_csv(os.path.join(dest_dir, 'detailed_summaries', '{}.csv'.format(kernel.name)))\n",
    "        if not kernel.tempered:\n",
    "            plot_circle(proposal_design[:,0], proposal_design[:,1], np.mean(np.exp(cur_sample), axis=0))\n",
    "            plt.savefig(os.path.join(dest_dir, kernel.name + '.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        if debug: print(log_likelihoods.ravel())\n",
    "        if debug: print(last_kernel_log_likes)\n",
    "        if debug: print(log_spatial_component)\n",
    "        if debug: print(log_temporal_component)\n",
    "        if debug: print(weights)\n",
    "        \n",
    "        # compute ESS\n",
    "        ess = np.sum(weights)**2 / np.sum(weights**2)\n",
    "        if debug: print(\"iter={} beta={} accept={} ESS={} Time={} name={}\".format(i, beta, average_acceptances.mean(), ess, taken, kernel.name))\n",
    "        \n",
    "        if average_acceptances.mean() < 0.2:\n",
    "            beta *= 0.75\n",
    "        res_df.loc[i] = [kernel.name, beta, average_acceptances.mean(), ess, taken]\n",
    "        res_df.to_csv(os.path.join(dest_dir, 'summary.csv'))\n",
    "        cur_sample = new_sample\n",
    "        samples.append(cur_sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transition kernels to use in the SMC procedure\n",
    "kernels = []\n",
    "for ix, i_data in enumerate(range(11,19)):\n",
    "    data_prev = np.empty((0,0)) if ix == 0 else data_dict[i_data-1]\n",
    "    data_next = data_dict[i_data]\n",
    "    \n",
    "    \n",
    "    for j, t in enumerate(np.linspace(0,1,intermediate_temperatures+2)[1:-1]):\n",
    "        tempered_kernel = eit.kernels.PCNTemperingKernel_C(\n",
    "            beta,\n",
    "            prior_mean,\n",
    "            sqrt_proposal_cov,\n",
    "            grid,\n",
    "            likelihood_variance,\n",
    "            pattern,\n",
    "            data_prev,\n",
    "            data_next,\n",
    "            t,\n",
    "            fun_args,\n",
    "            proposal_dot_mat\n",
    "        )\n",
    "        tempered_kernel.name = 'Frame_{}->{}_temp={}'.format(i_data-1, i_data, t)\n",
    "        tempered_kernel.tempered = True\n",
    "        kernels.append(tempered_kernel)\n",
    "    \n",
    "    pcn_kernel = eit.kernels.PCNKernel_C(beta, prior_mean, sqrt_proposal_cov, grid, likelihood_variance, pattern, data_next, fun_args, proposal_dot_mat)\n",
    "    pcn_kernel.name = 'Frame_{}'.format(i_data)\n",
    "    pcn_kernel.tempered = False\n",
    "    kernels.append(pcn_kernel)\n",
    "print(len(kernels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the SMC\n",
    "start = time.time()\n",
    "results = smc(initial_samples, kernels, 0.5, 0.2, False)\n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the output\n",
    "ix=2\n",
    "for i in range(intermediate_temperatures, len(results), intermediate_temperatures+1):\n",
    "    plt.figure()\n",
    "    plot_circle(proposal_design[:,0], proposal_design[:,1], np.mean(np.exp(results[i]), axis=0))\n",
    "    plt.title(kernels[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the output\n",
    "import pickle\n",
    "with open(os.path.join(dest_dir, 'results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "with open(os.path.join(dest_dir, 'duration.csv'), 'w') as f:\n",
    "    f.write(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
