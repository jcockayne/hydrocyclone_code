{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#from autograd import numpy as np\n",
    "import bayesian_pdes as bpdes\n",
    "from scipy import stats, linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "import os\n",
    "import time\n",
    "import eit\n",
    "import time\n",
    "%mcmc notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic, needs_local_scope\n",
    "@needs_local_scope\n",
    "@register_cell_magic\n",
    "def capture_cell(line, cell):\n",
    "    globals()[line.strip()] = cell\n",
    "    exec(cell, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dummy_data = False\n",
    "dummy_data_file = os.path.join('dummy_data', 'centered_blob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture_cell params\n",
    "# Parameters of the data\n",
    "scale = 1000 # Scaling multiplier applied to the observed data.\n",
    "stim_current = 1.0 # The stimulating current that is applied.\n",
    "likelihood_variance = 1.0 # The observation noise variance.\n",
    "\n",
    "# Parameters controlling the prior\n",
    "kernel_variance_mult = 1000.0 # The amplitude parameter on the prior covariance over the PDE solution.\n",
    "kernel_ls = 0.21109375 # The length-scale parameter on the prior covariance over the PDE solution.\n",
    "conductivity_variance_mult = 1.0 # The amplitude parameter on the prior covariance for the conductivity field.\n",
    "conductivity_ls = 0.3 # The length-scale parameter on the prior covariance for the conductivity field.\n",
    "\n",
    "\n",
    "# Parameters controlling the problem setup\n",
    "n_radii_forward = 7 # The number of radii used in constructing the set of design points for the forward problem.\n",
    "n_radii_inverse = 8 # The number of radii used in constructing the set of design points for the inverse problem.\n",
    "bayesian = True # Whether to use a Bayesian (PN) or non-Bayesian (symmetric collocation) forward solver\n",
    "include_prior_mean = False # Whether to include a prior mean, based on the best constant conductivity field.\n",
    "adapt_ls = False # Whether to include the length-scale as a hyperparameter or leave it fixed.\n",
    "run_time = 11 # The frame whose posterior we are attempting to sample from.\n",
    "\n",
    "# Sampling parameters\n",
    "n_repeats = 10000 # The number of times to repeat each application of the kernel.\n",
    "n_iter = 100 # Number of pCN iterations in a kernel application.\n",
    "thin = 100 # Sample thinning factor.\n",
    "restart = False # Whether to resume a previously completed chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Output Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = os.path.join('results', 'static_recovery_f{}_n={}'.format(run_time, n_radii_forward))\n",
    "if not bayesian:\n",
    "    dest_dir += '_collocation'\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dest_dir, 'params.py'), 'w') as f:\n",
    "    f.write(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accept = 0.1\n",
    "max_accept = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = np.loadtxt('recovered_rhos.csv', delimiter=',')\n",
    "baseline_xy = baseline[:,:2]\n",
    "baseline_fields = baseline[:,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important helpers!\n",
    "def plot_circle(x, y, values, cmap=None):\n",
    "    plt.tricontourf(x.ravel(), y.ravel(), values.ravel(), cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.legend()\n",
    "    plt.axis('scaled')\n",
    "    plt.xlim(-1.1,1.1); plt.ylim(-1.1,1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data file format is as follows:\n",
    "* Each row corresponds to a time\n",
    "* The first element of the row gives the time index of the measurement\n",
    "* The remaining columns give the measurements for each time\n",
    "* These are divided into seven blocks.\n",
    "    - Call the block $i$\n",
    "    - Within the block the measurements correspond to electrodes $j=1,\\dots,8$, $j\\neq i$\n",
    "    - Pair $(i,j)$ represents passing a current between electrode $i$ and electrode $j$ and measuring the voltage.\n",
    "    - The voltage is measured between the _reference_ electrode at $1$ and the electrode located at $1+i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for setting up the stim pattern\n",
    "stim_pattern = np.zeros((7,8))\n",
    "for i in range(7):\n",
    "    stim_pattern[i,0] = 1\n",
    "    stim_pattern[i, i+1] = -1\n",
    "stim_pattern = scale*stim_pattern*stim_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_pattern = np.zeros((7,8))\n",
    "meas_pattern[:,0] = 1\n",
    "meas_pattern[:, 1:] = np.diag(-np.ones(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = eit.grids.EITPattern(meas_pattern, stim_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/ST1trial3.DAT'\n",
    "raw_data = np.loadtxt(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(raw_data[:,1:].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(data):\n",
    "    res = {}\n",
    "    for r in data:\n",
    "        time = r[0]\n",
    "        other_data = r[1:]\n",
    "        res[time] = scale*other_data.reshape((7,7))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_to_dict(raw_data)\n",
    "# quick check to make sure the data dict matches expectation\n",
    "assert np.all(data_dict[1][0] == scale*raw_data[0][1:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = np.load(os.path.join(dummy_data_file, 'obs.npy'))\n",
    "dummy_true_field = np.load(os.path.join(dummy_data_file, 'theta.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_dict[run_time]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x, s_xbar, s_y, s_ybar = sp.symbols('x,xbar,y,ybar')\n",
    "a, a_x, a_y = sp.symbols('a,a_x,a_y')\n",
    "a_bar, a_x_bar, a_y_bar = sp.symbols('abar,a_xbar,a_ybar')\n",
    "s_length_scale, s_variance = sp.symbols('l,sigma')\n",
    "kernel = s_variance*sp.exp(-((s_x-s_xbar)**2 + (s_y-s_ybar)**2) / (2.*s_length_scale**2))\n",
    "symbols = [[s_x, s_y, a, a_x, a_y], [s_xbar, s_ybar, a_bar, a_x_bar, a_y_bar], [s_length_scale, s_variance]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use:\n",
    "\\begin{align*}\n",
    "    \\nabla \\cdot (\\exp(a)\\nabla u) &= \\nabla \\exp(a) \\cdot \\nabla u + \\exp(a) \\nabla^2 u \\\\\n",
    "    &= \\exp(a) \\left( \\nabla a \\cdot \\nabla u + \\nabla^2 u \\right) \\\\\n",
    "    &= \\exp(a) \\left( a_x u_x + a_y u_y + u_{xx} + u_{yy} \\right)\n",
    "\\end{align*}\n",
    "and:\n",
    "\\begin{align*}\n",
    "    \\exp(a) \\nabla u \\cdot n &= \\exp(a) (x u_x + y u_y)\n",
    "\\end{align*}\n",
    "for the special case of a unit circular domain centered at the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the collocation grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in_shell = n_radii_forward\n",
    "n_bdy = 32\n",
    "n_sensor = 8\n",
    "grid = eit.grids.construct_circular(n_in_shell, n_bdy, n_sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot design points\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(grid.interior[:,0], grid.interior[:,1], marker='x', color='black', s=10)\n",
    "plt.scatter(grid.boundary[:,0], grid.boundary[:,1], marker='x', color='black', s=10, label='Design point')\n",
    "plt.scatter(grid.sensors[:,0], grid.sensors[:,1], marker='x', color='green', label='Sensor')\n",
    "plt.xlim(1.05,-1.05);plt.ylim(1.05,-1.05)\n",
    "plt.legend(loc=2, bbox_to_anchor=(1.05,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Proposal Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_kernel = kernel\n",
    "def diff_x(k):\n",
    "    return k.diff(s_x)\n",
    "def diff_y(k):\n",
    "    return k.diff(s_y)\n",
    "def diff_x_bar(k):\n",
    "    return k.diff(s_xbar)\n",
    "def diff_y_bar(k):\n",
    "    return k.diff(s_ybar)\n",
    "\n",
    "proposal_op_system = bpdes.operator_compilation.sympy_gram.compile_sympy(\n",
    "    [diff_x, diff_y], \n",
    "    [diff_x_bar, diff_y_bar], \n",
    "    proposal_kernel,\n",
    "    [[s_x, s_y], [s_xbar, s_ybar], [s_length_scale, s_variance]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_design = eit.grids.construct_shell(np.linspace(0,1,n_radii_inverse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(proposal_design[:,0], proposal_design[:,1], marker='x')\n",
    "plt.xlim(1.05,-1.05);plt.ylim(1.05,-1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_design = np.row_stack([grid.interior_plus_boundary, grid.sensors])\n",
    "proposal_fun_args = np.array([conductivity_ls, conductivity_variance_mult])\n",
    "proposal_cov = bpdes.collocation.compute_operator_matrix(\n",
    "    [()],\n",
    "    [()],\n",
    "    [proposal_design],\n",
    "    [proposal_design],\n",
    "    proposal_op_system,\n",
    "    proposal_fun_args\n",
    ")\n",
    "proposal_lhs_mat = bpdes.collocation.compute_operator_matrix(\n",
    "    [(), diff_x, diff_y], \n",
    "    [()],\n",
    "    [full_design, grid.interior, grid.interior],\n",
    "    [proposal_design],\n",
    "    proposal_op_system,\n",
    "    proposal_fun_args\n",
    ")\n",
    "proposal_cov_inv = np.linalg.inv(proposal_cov)\n",
    "np.linalg.cond(proposal_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_lhs_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhomin = 0.01\n",
    "rho = baseline_fields[:,run_time-1][:,None]\n",
    "rho[rho < rhomin] = rhomin\n",
    "conduct = np.log(1./rho)\n",
    "p = bpdes.collocate([()], [()], \n",
    "                    [(baseline_xy, conduct)],\n",
    "                    proposal_op_system, \n",
    "                    np.array([0.12, 1.0]))\n",
    "\n",
    "true_field, _ = p(proposal_design)\n",
    "true_field = true_field.ravel()\n",
    "plot_circle(proposal_design[:,0], proposal_design[:,1], np.exp(true_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# * Swap to simulating only a lower dimensional version of the field\n",
    "proposal_dot_mat = np.dot(proposal_lhs_mat, proposal_cov_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = kernel_ls\n",
    "fun_args = np.array([kernel_ls, kernel_variance_mult])\n",
    "data = data_dict[run_time] if not use_dummy_data else dummy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an initial field\n",
    "import scipy.optimize\n",
    "def phi_const(k):\n",
    "    theta = k*np.ones(proposal_cov.shape[0])\n",
    "    return eit.kernels.c_kernel.phi(grid, theta, likelihood_variance, pattern, data, fun_args, proposal_dot_mat)\n",
    "    \n",
    "best_const = scipy.optimize.minimize_scalar(phi_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the proposal\n",
    "beta = 0.05\n",
    "if include_prior_mean:\n",
    "    prior_mean = best_const.x*np.ones(proposal_cov.shape[0])\n",
    "else:\n",
    "    prior_mean = np.zeros(proposal_cov.shape[0])\n",
    "sqrt_proposal_cov = np.real_if_close(linalg.sqrtm(proposal_cov))\n",
    "\n",
    "pcn_kernel = eit.kernels.PCNKernel_C(beta, prior_mean, sqrt_proposal_cov, grid, likelihood_variance, pattern, data, fun_args, proposal_dot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previous field if we are restarting\n",
    "if not restart:\n",
    "    kappa_0 = best_const.x*np.ones(proposal_cov.shape[0])\n",
    "else:\n",
    "    seed_file = os.path.join(dest_dir, 'results.npy')\n",
    "    kappa_0 = np.load(seed_file)[-1]\n",
    "\n",
    "beta_0 = 0.05    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_log_prior(ls):\n",
    "    if ls < 0:\n",
    "        return -np.inf\n",
    "    return scipy.stats.halfcauchy.logpdf(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now run the MCMC\n",
    "beta = beta_0\n",
    "per_repeat = int(n_iter/thin)\n",
    "res = np.empty((per_repeat*n_repeats, kappa_0.shape[0]))\n",
    "\n",
    "cur_length_scale = length_scale\n",
    "pcn_kernel.collocate_args = np.array([cur_length_scale, kernel_variance_mult])\n",
    "length_scales = np.empty(n_repeats)\n",
    "length_scale_proposals = np.empty(n_repeats)\n",
    "accepts = np.zeros(n_repeats, dtype=np.bool)\n",
    "length_scale_proposal_variance = 1e-3\n",
    "cur_ls_log_prior = ls_log_prior(length_scale)\n",
    "\n",
    "start = kappa_0.reshape((1, kappa_0.shape[0]), order='F')\n",
    "\n",
    "average_time = 0.\n",
    "for i in range(n_repeats):\n",
    "    init_time = time.time()\n",
    "    proposals, acceptances, log_likelihoods = pcn_kernel.apply(start, n_iter, n_threads=8, beta=beta, bayesian=bayesian)\n",
    "    took = time.time() - init_time\n",
    "    average_time = (average_time * (i) + took) / (i+1)\n",
    "    if acceptances.mean() < min_accept:\n",
    "        beta *= 0.75\n",
    "    elif acceptances.mean() > max_accept:\n",
    "        beta /= 0.75\n",
    "    if beta > 1:\n",
    "        beta = 1.\n",
    "    res[per_repeat*i:per_repeat*(i+1)] = proposals[::thin]\n",
    "    start = proposals[-1,:].reshape(start.shape, order='F')\n",
    "    \n",
    "    if adapt_ls:\n",
    "        # propose to change length_scale\n",
    "        new_length_scale = np.random.normal(cur_length_scale, length_scale_proposal_variance)\n",
    "        length_scale_proposals[i] = new_length_scale\n",
    "        new_ls_log_prior = ls_log_prior(new_length_scale)\n",
    "\n",
    "        if np.isneginf(new_ls_log_prior):\n",
    "            accept = False\n",
    "        else:\n",
    "            cur_phi = pcn_kernel.phi(start.ravel())\n",
    "            new_phi = pcn_kernel.phi(start.ravel(), np.array([new_length_scale, kernel_variance_mult]))\n",
    "            accept_prob = np.exp(cur_phi - new_phi + new_ls_log_prior - cur_ls_log_prior)\n",
    "            accept = np.random.uniform() < accept_prob\n",
    "        if accept:\n",
    "            cur_length_scale = new_length_scale\n",
    "            cur_ls_log_prior = new_ls_log_prior\n",
    "            pcn_kernel.collocate_args = np.array([cur_length_scale, kernel_variance_mult])\n",
    "    else:\n",
    "        accept = False\n",
    "    accepts[i] = accept\n",
    "    length_scales[i] = cur_length_scale\n",
    "    if i > 100:\n",
    "        if np.mean(accepts[i-99:i+1]) < min_accept:\n",
    "            length_scale_proposal_variance *= 0.75\n",
    "        elif np.mean(accepts[i-99:i+1]) > max_accept:\n",
    "            length_scale_proposal_variance /= 0.75\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print('{}: accept={} beta={} ls={} accept={} t/iter={:.2}s'.format(i, acceptances.mean(), beta, cur_length_scale, np.mean(accepts[:i+1]), average_time))\n",
    "#print log_likelihoods.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_circle(proposal_design[:,0], proposal_design[:,1], np.exp(np.mean(res[-1:], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n",
    "np.save(os.path.join(dest_dir, 'results.npy'), res)\n",
    "np.save(os.path.join(dest_dir, 'length_scales.npy'), length_scales)\n",
    "np.save(os.path.join(dest_dir, 'field_pts.npy'), proposal_design)\n",
    "\n",
    "plot_circle(proposal_design[:,0], proposal_design[:,1], np.exp(np.mean(res[40000:], axis=0)))\n",
    "plt.savefig(os.path.join(dest_dir, 'mean.pdf'), bbox_inches=0, transparent=True)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
